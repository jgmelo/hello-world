{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgmelo/hello-world/blob/master/notebooks/en/rag_with_hf_and_milvus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QBAlp4oV-M4"
      },
      "source": [
        "# Build RAG with Hugging Face and Milvus\n",
        "\n",
        "_Authored by: [Chen Zhang](https://github.com/zc277584121)_\n",
        "\n",
        "\n",
        "[Milvus](https://milvus.io/) is a popular open-source vector database that powers AI applications with highly performant and scalable vector similarity search. In this tutorial, we will show you how to build a RAG (Retrieval-Augmented Generation) pipeline with Hugging Face and Milvus.\n",
        "\n",
        "The RAG system combines a retrieval system with an LLM. The system first retrieves relevant documents from a corpus using Milvus vector database, then uses an LLM hosted in Hugging Face to generate answers based on the retrieved documents."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls -lh"
      ],
      "metadata": {
        "id": "m3M75OKBWWtn"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "IiQA_cuuV-M-"
      },
      "source": [
        "## Preparation\n",
        "### Dependencies and Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "collapsed": true,
        "id": "HzIa6Yl3V-M-"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade pymilvus sentence-transformers huggingface-hub langchain_community langchain-text-splitters pypdf tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "GD_qIbxHV-NA"
      },
      "source": [
        "> If you are using Google Colab, to enable the dependencies, you may need to **restart the runtime** (click on the \"Runtime\" menu at the top of the screen, and select \"Restart session\" from the dropdown menu)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "F45hpVnyV-NA"
      },
      "source": [
        "In addition, we recommend that you configure your [Hugging Face User Access Token](https://huggingface.co/docs/hub/security-tokens), and set it in your environment variables because we will use a LLM from the Hugging Face Hub. You may get a low limit of requests if you don't set the token environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "2O6-FSZzV-NB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCG2WHlcV-NC"
      },
      "source": [
        "### Prepare the data\n",
        "\n",
        "We use the [AI Act PDF](https://artificialintelligenceact.eu/wp-content/uploads/2021/08/The-AI-Act.pdf), a regulatory framework for AI with different risk levels corresponding to more or less regulation, as the private knowledge in our RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        },
        "collapsed": true,
        "id": "0-wH7x7rV-NC"
      },
      "outputs": [],
      "source": [
        "# %%bash\n",
        "\n",
        "# if [ ! -f \"The-AI-Act.pdf\" ]; then\n",
        "#     wget https://artificialintelligenceact.eu/wp-content/uploads/2021/08/The-AI-Act.pdf\n",
        "# fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3ed8HVSV-ND"
      },
      "source": [
        "We use the [`PyPDFLoader`](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/) from LangChain to extract the text from the PDF, and then split the text into smaller chunks. By default, we set the chunk size as 1000 and the overlap as 200, which means each chunk will nearly have 1000 characters and the overlap between two chunks will be 200 characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "collapsed": true,
        "id": "exf5ZBiYV-ND",
        "outputId": "c5dad40a-3167-4b7d-af2c-910c00b37308",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1174\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# loader = PyPDFLoader(\"The-AI-Act.pdf\")\n",
        "loader = PyPDFLoader(\"Static Timing Analysis for Nanometer Designs.pdf\")\n",
        "docs = loader.load()\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Vq0GdsYkV-NF"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "ms4gqQYHV-NF"
      },
      "outputs": [],
      "source": [
        "text_lines = [chunk.page_content for chunk in chunks]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of chunks: {text_lines}')"
      ],
      "metadata": {
        "id": "JPsaxut_mHAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibibUluGV-NG"
      },
      "source": [
        "### Prepare the Embedding Model\n",
        "Define a function to generate text embeddings. We use [BGE embedding model](https://huggingface.co/BAAI/bge-small-en-v1.5) as an example, but you can use any embedding models, such as those found on the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "collapsed": true,
        "id": "XDFnecm6V-NG"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "def emb_text(text):\n",
        "    return embedding_model.encode([text], normalize_embeddings=True).tolist()[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvnedmXGV-NG"
      },
      "source": [
        "Generate a test embedding and print its dimension and first few elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "6geuZZGgV-NH",
        "outputId": "648e1eba-3a61-4f01-f7f2-bc792c29a7bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "384\n",
            "[-0.07660680264234543, 0.02531672641634941, 0.012505538761615753, 0.004595162346959114, 0.02577998675405979, 0.038167111575603485, 0.08050814270973206, 0.0030353872571140528, 0.024392176419496536, 0.004880355205386877]\n"
          ]
        }
      ],
      "source": [
        "test_embedding = emb_text(\"This is a test\")\n",
        "embedding_dim = len(test_embedding)\n",
        "print(embedding_dim)\n",
        "print(test_embedding[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tEqC77eV-NH"
      },
      "source": [
        "## Load data into Milvus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQPQOl-4V-NH"
      },
      "source": [
        "### Create the Collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "F2irtIsgV-NI",
        "outputId": "5a59c0cd-90b3-4975-84b3-48da9f6ae793",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:pymilvus.milvus_client.milvus_client:Created new connection using: a0e04b3494cc49718e011d785d2594a3\n"
          ]
        }
      ],
      "source": [
        "from pymilvus import MilvusClient\n",
        "\n",
        "milvus_client = MilvusClient(uri=\"./hf_milvus_demo.db\")\n",
        "\n",
        "collection_name = \"rag_collection\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "e2zJro3cV-NI"
      },
      "source": [
        "> As for the argument of `MilvusClient`:\n",
        "> - Setting the `uri` as a local file, e.g.`./hf_milvus_demo.db`, is the most convenient method, as it automatically utilizes [Milvus Lite](https://milvus.io/docs/milvus_lite.md) to store all data in this file.\n",
        "> - If you have a large amount of data, say more than a million vectors, you can set up a more performant Milvus server on [Docker or Kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server uri, e.g.`http://localhost:19530`, as your `uri`.\n",
        "> - If you want to use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the `uri` and `token`, which correspond to the [Public Endpoint and Api key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE-gCkMCV-NI"
      },
      "source": [
        "Check if the collection already exists and drop it if it does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "fNuS9a32V-NI"
      },
      "outputs": [],
      "source": [
        "if milvus_client.has_collection(collection_name):\n",
        "    milvus_client.drop_collection(collection_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3TMFD96V-NJ"
      },
      "source": [
        "Create a new collection with specified parameters.\n",
        "\n",
        "If we don't specify any field information, Milvus will automatically create a default `id` field for primary key, and a `vector` field to store the vector data. A reserved JSON field is used to store non-schema-defined fields and their values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "eb0WRT8KV-NJ",
        "outputId": "1c7bcf23-f559-4546-b0d4-ba3fee0876a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:pymilvus.milvus_client.milvus_client:Successfully created collection: rag_collection\n",
            "DEBUG:pymilvus.milvus_client.milvus_client:Successfully created an index on collection: rag_collection\n"
          ]
        }
      ],
      "source": [
        "milvus_client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    dimension=embedding_dim,\n",
        "    metric_type=\"IP\",  # Inner product distance\n",
        "    consistency_level=\"Strong\",  # Strong consistency level\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y55Lk_aaV-NJ"
      },
      "source": [
        "### Insert data\n",
        "Iterate through the text lines, create embeddings, and then insert the data into Milvus.\n",
        "\n",
        "Here is a new field `text`, which is a non-defined field in the collection schema. It will be automatically added to the reserved JSON dynamic field, which can be treated as a normal field at a high level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "xERmZXRqV-NJ",
        "outputId": "ed1ce345-7a31-44df-b1ee-64a49c6f704b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating embeddings: 100%|██████████| 2096/2096 [00:28<00:00, 73.63it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2096"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "data = []\n",
        "\n",
        "for i, line in enumerate(tqdm(text_lines, desc=\"Creating embeddings\")):\n",
        "    data.append({\"id\": i, \"vector\": emb_text(line), \"text\": line})\n",
        "\n",
        "insert_res = milvus_client.insert(collection_name=collection_name, data=data)\n",
        "insert_res[\"insert_count\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOB7b5uJV-NK"
      },
      "source": [
        "## Build RAG\n",
        "\n",
        "### Retrieve data for a query\n",
        "\n",
        "Let's specify a question to ask about the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "KHh4L5huV-NK"
      },
      "outputs": [],
      "source": [
        "question = \"List a few considerations to be accounted to constrain a design.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7yBQ_xmV-NK"
      },
      "source": [
        "Search for the question in the collection and retrieve the top 3 semantic matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "S7iRXjAEV-NK"
      },
      "outputs": [],
      "source": [
        "search_res = milvus_client.search(\n",
        "    collection_name=collection_name,\n",
        "    data=[\n",
        "        emb_text(question)\n",
        "    ],  # Use the `emb_text` function to convert the question to an embedding vector\n",
        "    limit=3,  # Return top 3 results\n",
        "    search_params={\"metric_type\": \"IP\", \"params\": {}},  # Inner product distance\n",
        "    output_fields=[\"text\"],  # Return the text field\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zP9mEeFV-NK"
      },
      "source": [
        "Let's take a look at the search results of the query\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "C87ifez0V-NK",
        "outputId": "16c7d0e6-f465-40c7-de3f-05f9c7d9cea8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    [\n",
            "        \"1.4.3 Asynchronous Designs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n1.5 STA at Different Design Phases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n1.6 Limitations of Static Timing Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n1.7 Power Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n1.8 Reliability Considerations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n1.9 Outline of the Book. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\nCHAPTER 2: STA Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.1 CMOS Logic Design. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.1.1 Basic MOS Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.1.2 CMOS Logic Gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\",\n",
            "        0.6359909176826477\n",
            "    ],\n",
            "    [\n",
            "        \"1.4.3 Asynchronous Designs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n1.5 STA at Different Design Phases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n1.6 Limitations of Static Timing Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n1.7 Power Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n1.8 Reliability Considerations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n1.9 Outline of the Book. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\nCHAPTER 2: STA Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.1 CMOS Logic Design. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.1.1 Basic MOS Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.1.2 CMOS Logic Gate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\",\n",
            "        0.6359909176826477\n",
            "    ],\n",
            "    [\n",
            "        \"C H A P T E R\\n7\\nConfiguring the STA\\nEnvironment\\nhis chapter describes how to set up the environment for static timing\\nanalysis. Specification of correct constraints is important in analyzing\\nSTA results. The design environment should be specified accurately\\nso that STA analysis can identify all the timing issues in the design. Prepar-\\ning for STA involves amongst others, setting up clocks, specifying IO tim-\\ning characteristics, and specifying false paths and multicycle paths. It is\\nimportant to understand this chapter thoroughly before proceeding with\\nthe next chapter on timing verification.\\nT\\n J. Bhasker and R. Chadha, Static Timing Analysis for Nanometer Designs: A Practical Approach, 179\\nDOI: 10.1007/978-0-387-93820-2_7, \\u00a9 Springer Science + Business Media, LLC 2009\",\n",
            "        0.6317266225814819\n",
            "    ]\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "retrieved_lines_with_distances = [\n",
        "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
        "]\n",
        "print(json.dumps(retrieved_lines_with_distances, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y96wihpV-NL"
      },
      "source": [
        "### Use LLM to get an RAG response\n",
        "\n",
        "Before composing the prompt for LLM, let's first flatten the retrieved document list into a plain string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "NQT88k51V-NL"
      },
      "outputs": [],
      "source": [
        "context = \"\\n\".join(\n",
        "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc2dINO9V-NL"
      },
      "source": [
        "Define prompts for the Language Model. This prompt is assembled with the retrieved documents from Milvus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "CrGfPwuxV-NM"
      },
      "outputs": [],
      "source": [
        "PROMPT = \"\"\"\n",
        "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "<question>\n",
        "{question}\n",
        "</question>\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWeeCy0GV-NM"
      },
      "source": [
        "We use the [Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) hosted on Hugging Face inference server to generate a response based on the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "DVYymoFLV-NM"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "\n",
        "llm_client = InferenceClient(model=repo_id, timeout=120)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "ERRsQUPhV-NM"
      },
      "source": [
        "Finally, we can format the prompt and generate the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rzeTHHxkV-NS"
      },
      "outputs": [],
      "source": [
        "prompt = PROMPT.format(context=context, question=question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn4KIGuHV-NS",
        "outputId": "8618b52e-57ff-41bc-f76f-8b218ba86d23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<answer>\n",
            "A few considerations to be accounted to constrain a design include setting up clocks, specifying IO timing characteristics, and specifying false paths and multicycle paths.\n",
            "</answer>\n",
            "</response>\n"
          ]
        }
      ],
      "source": [
        "answer = llm_client.text_generation(\n",
        "    prompt,\n",
        "    max_new_tokens=1000,\n",
        ").strip()\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "qfap9WDCV-NS"
      },
      "source": [
        "Congratulations! You have built an RAG pipeline with Hugging Face and Milvus."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GTXWeQdsKfy9"
      },
      "execution_count": 118,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}